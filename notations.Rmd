---
#title: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```

#### Goal

Clarify the connection between causal graphs and Bayesian networks (and their associated estimation procedures). Emphasis on the case of time varying treatments/ time varying confounders, for which marginal structural models/ IP weighting is the ubiquitous estimation method. Ideally, offer a Bayesian (network) approach to this problem and apply to Multicenter AIDS Cohort Study (MACS) and compare results with MSM.

- General introduction to causal inference/ MSM + IP weighting
- General introduction to Bayesian causal inference (conditioning versus intervention)
- Bayesian network representation/ solution to single time point treatment/ causal estimand
- Bayesian network representation/ solution to time varying case 
- Data analysis


#### Causal Inference Notation: Counterfactual RVs and Causal Effect

Let $A$ denote a treatment and $Y$ an outcome. For simplicity, assume both are binary. In order to define a causal effect, decompose $Y$ into two new random variables, **counterfactuals**: $Y^1$ is the outcome of the subject under treatment $1$ and $Y^0$ is the outcome of the subject under treatment $0$. When $A=1$, $Y^0$ is unobserved (and vice-versa). With these new random variables, we can define causal effects by comparing outcomes under treatments; for instance, the average causal effect is $\theta = E(Y^1) - E(Y^0)$.

#### Randomized Experiments

Now let us explore these concepts a little bit further in the setting of randomized experiments. Let $\mathcal{A}= \{a,a',a'', \ldots \}$ denote the full set of treatment values and $Y^{\mathcal{A}} = \{Y^a, Y^{a'},Y^{a''}, \ldots \}$ denote the full set of counterfactual outcomes. The key characteristic of randomization is that the set of counterfactual outcomes are jointly independent of treatment, $Y^{\mathcal{A}} \perp A$. This is referred to as **exchangeability**. To distinguish this from the observational scenario, we also refer to this as **marginal exchangeability**

Continuing the example above, exchangeability implies that $P(Y^1 = 1 \vert A=0) = P(Y^1 = 1 \vert A=1)$. So, the risk under treatment level $1$ does not depend on the treatment the individual actually received. This is achieved by the random assignment of treatment.

The second key condition needed to estimate causal effects is **consistency**. For any individual, we observe only one of their counterfactuals. For an individual who received treatment level $a'$, we observe $Y^{a'}$. This idea is formalized with the consistency condition, which requires that values of treatment under comparison correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data. That is. $Y^{a'}=Y$ when treatment $a'$ has been observed, where $Y$ is the observed value of the outcome.

These two conditions allow us to use the *ordinary* tools of statistics (here, estimating conditional expectation functions) and observed data to consistently estimate causal effects. In particular,

$$
\begin{aligned}
E(Y \vert A=a) &= E(Y^{a} \vert A=a) & \text{consistency} \\
 &= E(Y^{a}) & \text{exchangeability} \\
\end{aligned}
$$

The situation is slightly complicated in observational studies.


#### Observational Studies

In observational studies, treatment is not randomly assigned. In particular, there may be variables that have an effect on both the observed treatment level and the outcome of interest. These variables are typically referred to as confounders. However, if we have data on these variables, let us denote them $L$, then we can achieve **conditional exchangeability**, in which:

$P(Y^a=y|A=a,L)=P(Y^a=y|A=a',L)$. That is, $Y^a \perp A | L$ for all $a \in \mathcal{A}$.

Or, within levels of $L$, counterfactual outcomes are independent of the treatment received. Another way to think of this is that we effectively have data from a randomized experiment within levels of $L$.

Therefore, we can again use exchangeability and consistency to translate between counterfactuals and conditional expectations,

$$
\begin{align}
E(Y \vert A=a, L) &= E(Y^{a} \vert A=a, L) & \text{consistency} \\
 &= E(Y^{a} \vert L) & \text{exchangeability} \\
\end{align}
$$

Suppose, as before, we aim to estimate the average causal effect, $\theta = E(Y^{a'}) - E(Y^{a})$ (this time the effect pertains to the comparison between treatment levels $a$ and $a'$). Then we can use the law of total expectations to attain each term,

$$
\begin{aligned}
E(Y^{a}) &= \sum_lE(Y^{a} \vert L) P(L) \\
&= \sum_lE(Y \vert A=a, L) P(L)
\end{aligned}
$$

Ok, so how can we estimate this quantity?


#### 


#### IP Weighting

Propensity score weighting is ubiquitous in causal inference, due mainly to its balancing property (treatment is independent of observable covariates given propensity score). There are many implementations. We focus on IP weighting/ marginal structural models since these are the most prevalent for the time varying case (which we aim to approach from a Bayesian (network) perspective).

Let's consider the DAG structure corresponding to the simple observational scenario (single time point and confounder $L$) outlined above. 

```{r echo = F, message=F, warning=F}
library(igraph)
edges = c(1,2,1,3,2,3)
g = make_graph(edges) %>% set_vertex_attr("label", value = c("L", "A", "Y"))
plot(g)
```

We consider each variable as random. The joint distribution can be expressed,

$$
\begin{aligned}
P(L,A,Y) = P(L) P(A|L) P(Y|A,L) \\
\end{aligned}
$$

We aim to estimate $E(Y^{a'})$. Consider the following quantity (for a single unit/ subject),

$$
\begin{aligned}
\frac{{1}(A=a')Y} {P(A|L)} \\
\end{aligned}
$$

where the observed value of the outcome under treatment $a'$ is weighted by the treatment probability given covariates $L$. Provided that we have correctly specified the treatment model, $P(A \vert L)$, this quantity is an unbiased estimator of the mean counterfactual outcome since,

$$
\begin{aligned}
E \bigg( \frac {1(A=a')Y} {P(A \vert L)} \bigg) &= \sum_y \sum_l \sum_a \bigg( \frac{ {1}(A=a')y} {P(A|L)} \bigg) P(L=l,A=a,Y=y) \\
&= \sum_y \sum_l \bigg( \frac{ y} {P(A=a'|L)} \bigg) P(L=l) P(A=a' \vert L) P(Y=y|A=a',L) & \text{expression is 0 when } A \neq a'  \\
&= \sum_l \sum_y y P(Y|A=a',L=l) P(L=l) & \text{cancellation of } P(A=a' \vert L)  \\
&= \sum_l E(Y \vert A=a', L=l) P(L=l) \\
&= E(Y^{a'}) & \text{consistency + exchangeability}
\end{aligned}
$$

#### Estimation and Marginal Structural Models

Therefore, we can estimate $E(Y^{a'})$ by taking a weighted mean of the outcomes observed at treatment $a'$ where each unit is weighted by $1/P(A=a'|L)$. Let us consider a more general framework to carry out this computation using an estimating equation. Let us again consider the scenario where we have a binary treatment and let us refer to $E(Y^{1})$ as $\mu^1$. Further, we assume that we have $n$ experimental units/ subjects and we aim to estimate the causal risk difference, $\theta = \mu^1 - \mu^0$. To estimate the first quantity, $\mu^1$, we can solve the following estimating equation,

$$
\begin{equation}
\label{eq:1}
U(\mu^1) = \sum_{i=1}^n \frac{1(a_i=1)(y_i-\mu^1)}{p(a_i \vert l_i)}
\end{equation}
$$
where the sum is taken over each subject/ unit. Solving for $\mu^1$, we get the following estimator,

$$
\begin{align}
\hat{\mu^1} &=  \frac{\sum_t y_t/p(a_t \vert l_t)} {\sum_t 1/p(a_t \vert l_t)} \\
\end{align}
$$
where the sum is taken over the treated units, $t=\{i : a_i=1 \}$. This procedure (inverse probability weighting) can be interpreted/ conceptualized as generating a pseudo population in which treatment assignments do not depend on $L$. Therefore, in this re-weighted sample, we have marginal exchangeability and can estimate the casual risk difference as the association risk difference among this pseudo population. That is, if we refer to the re-weighted response as $\tilde{Y}$, then $\theta= E(\tilde{Y} \vert A=1) - E(\tilde{Y} \vert A=0)$.

#### Many treatments (Marginal Structural Models)

In the case where there are many treatment levels or treatment is continuous, it makes practical sense to hypothesize a functional relationship between the treatment and response. For example, a linear relationship can be expressed,

$$
\begin{aligned}
E(Y^{a}) = \beta_0 + \beta_1 a
\end{aligned}
$$
Where $\beta_1$ can be interpreted as the causal effect of increasing treatment by one unit. More generally, we can refer to this mean function as $\mu(\boldsymbol{\beta})$. In this more general scenario, the coefficients can be estimated using the *weighted* estimating equation,

$$
\begin{aligned}
U(\beta) &=  A^{\top}W (y-\mu(\boldsymbol{\beta}))
\end{aligned}
$$

in which $W$ is a diagonal matrix in which $w_{ii} = 1/p(A_i \vert L_i)$ and $A$ is our design matrix which includes the treatment assignment for each unit. In the case of a binary treatment/ binary outcome, this reduces to the case above.


#### Pseudopopulation Concept

One way to conceptualize the weights generated by IPW is a pseudo population in which all sample units undergo each of the treatment levels of interest. Within the $L=0$ segment, there are $8$ units, $4$ of whom were treated. Of the treated $1/4$ died. How many would have died if all of the units with $L=0$ had been treated? By conditional exchangeability, we would expect the same rate of death among the untreated, which would imply one more death had all of the units undergone treatment ($1/4 \times 4$). 

```{r echo=FALSE}

L = c(0,0,0,0,1,1,1,1)
A = c(0,0,1,1,0,0,1,1)
Y = c(0,1,0,1,0,1,0,1)
N = c(3,1,3,1,1,2,3,6)
dat = data.frame(L=L, A=A, Y=Y, N=N)

mw = glm(A ~ L, family = binomial, weights = N, data = dat) # weight model (P(a | l))
w = 1/ifelse(A==1, fitted.values(mw), 1 - fitted.values(mw)) # w
N_pseudo = w * N

#dat$w = w
dat$N_prime = w * N
kable(dat)

# OK - how many died if all of the L = 0 were treated
# Pr(Y=1 | )

```

By inspection, we can see that within levels of $L$, the effect of treatment is $0$. Therefore the causal parameter $\theta$ should be $0$. However, the association risk difference differs from the causal risk difference due to confounding by $L$. That is, $E(Y \vert A=1) - E(Y \vert A=0) \neq  E(Y^1) - E(Y^0)$. However, we can estimate the causal risk difference by weighting. In particular, each unit $i$ is given a weight $w_i = 1/p(a_i \vert l_i)$. These weighted units are sometimes called a 'pseudo population' and this weighted population is not confounded by $L$. So, we can estimate the causal effect by comparing the conditional risk differences in the *pseudopopulation*,

```{r}
theta = sum(ifelse(A==1, N_pseudo * Y, 0))/sum(ifelse(A==1, N_pseudo, 0)) - sum(ifelse(A==0, N_pseudo * Y, 0))/sum(ifelse(A==0, N_pseudo, 0))
```

As expected, this gets us $\hat{\theta} =$ `r round(theta,1)`. 


#### Example MSM Estimation

Consider a scenario where we aim to estimate the effect of a binary treatment, $A$, on a continuous outcome, $Y$, using observational data. Let's suppose we also have data on each patients' health, $L = \{1= \text{poor}, 2= \text{medium}, 3 = \text{good} \}$ and that $Y$ is a measure of cholesterol. Suppose that patients with better health are less likely to get treatment and that better health causes lower cholesterol. We want to estimate the population average causal effect, $\theta = E(Y^1) - E(Y^0)$ (i.e., no conditioning on $L$). The *true* value of $\theta$ is $-10$.

```{r echo=TRUE}

set.seed(1)
n = 1000 # number of subjects
L = sample.int(3, size = n, replace = T, prob = c(1, 1, 1)) # health level (higher is better)
A = rbinom(n, 1, prob = plogis(2-L)) # healthier less likely to get treatment
Y = rnorm(n, mean = 200 - 10 * A - 20 * L) 

# simple linear model, no conditioning on L
beta1.m0 = lm(Y ~ A)$coefficients[2]

# P(A|L)
tm = glm(A ~ L, family = binomial)
pa1 = predict(tm, type = "response") # P(A=1 | L)
w = ifelse(A==1, pa1, 1-pa1) # P(A=a | L)  = W

mod = lm(Y ~ A, weights = 1/w)
beta1.m1 = coefficients(mod)[2]
```

The simple estimator,

$$
\begin{aligned}
\hat{\alpha} = \hat{E}(Y\vert A=1) - \hat{E}(Y\vert A=0)
\end{aligned}
$$

yields $\hat{\alpha}=$ `r round(beta1.m0,2)`. This would indicate that the treatment *increases* cholesterol. This bias arises since less healthy people with higher cholesterol are the ones who get treatment. On the other hand, the IPW derived estimate of the population average causal effect is `r round(beta1.m1,2)`.


#### Variance of IPTW Estimator

As outlined above, the IPTW estimation procedure involves two steps: first, estimate $P(A|L)$ and, second, use these estimates as weights in a GEE. Thus, the variance of the coefficients in the marginal structural model must account for the variability in the estimation of the weights. We can evaluate the asymptotic variance of the MSM coefficients by partitioning the parameter vector.

We assume a GLM model for $P(A|L)$ with parameter vector $\boldsymbol{\alpha}$. Thus, we can write the score function as $U(\boldsymbol{\alpha})=L^T(A-\mu(\boldsymbol{\alpha}))$. To proceed, we package up all of our parameters into a vector, $\boldsymbol{\theta}=(\boldsymbol{\alpha}, \boldsymbol{\beta})$. Then, we define a score function,

\begin{equation*}
    U(\boldsymbol{\theta}) = \begin{bmatrix}
    U_1(\boldsymbol{\alpha}) \\
    U_2(\boldsymbol{\alpha}, \boldsymbol{\beta}) \\
    \end{bmatrix} = 
    \begin{bmatrix}
    L^T(a-\mu(\boldsymbol{\alpha})) \\
    A^T W(\boldsymbol{\alpha})(y-\mu(\boldsymbol{\beta})) \\
    \end{bmatrix}
\end{equation*}

where $U_1$ corresponds with the score function for our model of $P(A|L)$ and $U_2$ is the score function developed above, making explicit that W depends on $\boldsymbol{\alpha}$. Under some regularity conditions and the assumption that the treatment model $P(A|L)$ is correctly specified, $E(U(\boldsymbol{\theta}))=0$. Thus, estimators for $\hat{\boldsymbol{\theta}}$ can be found by finding the values for $(\hat{\boldsymbol{\alpha}}, \hat{\boldsymbol{\beta}})$ such that $U(\hat{\boldsymbol{\theta}})=0$. Or, more practically, we first can estimate $\boldsymbol{\alpha}$ then use this estimate in $U_2$ to estimate $\boldsymbol{\beta}$. Furthermore, the asymptotic variance of $\hat{\boldsymbol{\theta}}$ can be expressed,

\begin{align*}
    V(\hat{\boldsymbol{\theta}}) &=
    E \bigg( \frac {\partial U} {\partial \boldsymbol{\theta}} \bigg) ^{-1}
    E(U(\boldsymbol{\theta})U(\boldsymbol{\theta})^T)
    E \bigg( \frac {\partial U} {\partial \boldsymbol{\theta}^T} \bigg) ^{-1}
     \\
\end{align*}




