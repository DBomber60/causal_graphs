---
#title: "Bayesian CI"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

### Bayesian Causal Inference

#### Potential Outcomes Framework

Let us again consider a fully randomized experiment with binary treatment, $A$, and binary outcome $Y$. Under the potential outcomes framework, there are three quantities associated with each unit, 

$$
(Y_i^0, Y_i^1, A_i)
$$

where $Y_i^1$ is the response of unit $i$ under treatment. We assume that our data consists of a sample of $N$ units from a target population. Two of these quantities are observed,

$$
(Y_i^{obs}=Y_i^{A_i}, A_i)
$$

And one is missing: $Y^{mis}=Y_i^{1-A_i}$. Therefore, given $A_i$, there is a $1:1$ map betwen, $(Y_i^1, Y_i^0)$ and $(Y_i^{obs}, Y_i^{miss})$.

Bayesian inference considers the observed values of these quantities to be realizations of random variables and the unobserved values to be unobserved random variables, just like other model parameters. Bayesian causal inference typically proceeds by assuming that the joint distribution of the **complete** data (i.e., all counterfactual outcomes) depends on some parameters, $\Theta$, and we can express the joint distribution as,

$$
\begin{align}
P(Y_i^0, Y_i^1, A_i \vert \Theta) = P(A_i \vert Y_i^0, Y_i^1, \Theta) P(Y_i^0, Y_i^1 \vert A_i, \Theta)
\end{align}
$$
Then, we make the following two assumptions,

1. $(Y_i^1, Y_i^0) \perp A_i$ (marginal exchangeability)
2. $\Theta = (\theta_Y, \theta_A)$, where the two parameter sets are apriori distinct and independent

Which simplifies the above expression of the joint distribution to,

$$
\begin{align}
P(Y_i^0, Y_i^1, A_i \vert \Theta) = P(A_i \vert \theta_A) P(Y_i^0, Y_i^1 \vert \theta_Y).
\end{align}
$$

Our causal estimand can be related to the sample or population. For example, the sample average treatment effect (SATE) can be expressed,

$$
\tau^S = \frac{1}{N} \sum_i \{Y_i^1-Y_i^0 \}.
$$
Or, it can be related to the population,

$$
\tau^P = E\{Y_i^1-Y_i^0 \}.
$$

A Gibbs sampling approach to causal inference consists of iteratively sampling $(Y^{miss}, \theta^Y)$ from $P(Y^{miss} \vert Y^{obs},A,\theta_Y)$ and $P(\theta^Y \vert Y^{miss}, Y^{obs}, A)$.

#### Example

Randomized experiment with binary outcome binary treatment. Since the joint potential distribution of the potential outcomes can take on one of four values, we can use a multinomial model for $(Y_i^1, Y_i^0)$.


$$
\begin{align}
(\pi_{11}, \pi_{10}, \pi_{01}, \pi_{00}) &\sim \text{Dir}(\alpha_{11},\alpha_{10},\alpha_{01},\alpha_{00}) \\
(Y^1,Y^0) &\sim \text{MN}(\pi_{11}, \pi_{10}, \pi_{01}, \pi_{00})
\end{align}
$$


$$
\begin{align}
P(Y^{miss} \vert Y^{obs}, A, \theta_Y) &\propto \prod_{i:A_i=1} P(Y_i^0 \vert Y_i^1, \theta_Y) \prod_{i:A_i=0} P(Y_i^1 \vert Y_i^0, \theta_Y) \\
 &=     \begin{cases}
      \text{Bern}(\pi_{11}/(\pi_{11}+\pi_{10})) & \text{if } A_i = 1, Y_i^{obs}=1\\
      \text{Bern}(\pi_{01}/(\pi_{01}+\pi_{00})) & \text{if } A_i = 1, Y_i^{obs}=0 \\
      \text{Bern}(\pi_{11}/(\pi_{01}+\pi_{11})) & \text{if } A_i = 0, Y_i^{obs}=1\\
      \text{Bern}(\pi_{10}/(\pi_{10}+\pi_{00})) & \text{if } A_i = 0, Y_i^{obs}=0 \\

    \end{cases}    \\
P(\theta_Y \vert Y^{obs}, Y^{miss}) &\sim \text{Dir} (\alpha_{11}+N_{11}, \alpha_{10}+N_{10}, \alpha_{01}+N_{01}, \alpha_{00}+N_{00})
\end{align}
$$



#### Suggested Refinement

The Bayesian framework makes clear the connection between missing data problems and causal inference. In the case of this example, we can think of the counterfactuals as a bivariate response with missing entries. What distinguishes causal inference from the *usual* cases of missing (multivariate) data is that the multivariate outcomes are never observed simultaneously. Therefore, while we have informatino on the marginal distributions, there is no data on the association among outcomes.




$$
\begin{align}
\pi_{1+} &= \pi_{11}+\pi_{10} = P(Y^1=1) \\
\pi_{+1} &= \pi_{01}+\pi_{11} = P(Y^0=1) \\
\gamma &= \frac{P(Y^1=1 \vert Y^0=1)}{P(Y^1=1 \vert Y^0=0)}
\end{align}
$$


$$
\begin{align}
P(Y^0=1 \vert Y^1=1, L=1) &\sim \text{Bern} \bigg( \frac{\gamma \pi_{+1}}{1-\pi_{+1}+\gamma \pi_{+1}} \bigg) \\
P(Y^0=1 \vert Y^1=0, L=1) &\sim \text{Bern} \bigg( \frac{\pi_{+1}(1-\pi_{+1})+\gamma(\pi_{+1}-\pi_{1+})}{(1-\pi_{1+})(1-\pi_{+1}+\gamma \pi_{+1})} \bigg) \\
P(Y^1=1 \vert Y^0=0, L=0) &\sim \text{Bern} \bigg(\frac{\pi_{1+}}{1-\pi_{+1}+\gamma \pi_{+1}}  \bigg) \\
P(Y^1=1 \vert Y^0=1, L=0) &\sim \text{Bern} \bigg( \frac{\gamma \pi_{1+}}{1-\pi_{+1}+\gamma \pi_{+1}} \bigg) \\
\end{align}
$$
#### Simulation Study

```{r}
set.seed(1)
n = 500
py0 = .3
py1 = .5
gam = 1

py0y11 = (gam * py0) / (1-py0 + gam*py0) # P(Y0=1 | Y1 = 1)
py0y10 = (py0*(1-py0+gam*(py0-py1)))/((1-py1)*(1-py0 + gam * py0)) # P(Y0=1 | Y1 = 0)
py1y01 = (gam * py1) / (1-py0 + gam*py0) # P(Y1=1 | Y0=1)
py1y00 = (py1) / (1 - py0 + gam*py0) # P(Y1=1 | Y0=0)

# first, lets simulate potential outcomes
Y0 = rbinom(n, 1, prob = py0)
prob_vec = ifelse(Y0==0, py1y00, py1y01)
Y1 = rbinom(n, 1, prob = prob_vec)

# treatment indicator
A = rbinom(n, 1, prob = .5)

# observed data
outcomes = cbind(Y0, Y1)

dat = data.frame(cbind(outcomes, A))

dat$Y0 = ifelse(A == 0, Y0, NA)
dat$Y1 = ifelse(A == 1, Y1, NA)

options(knitr.kable.NA = '-')
kable(head(dat))

```



```{r}
library(gtools)

nIter = 100 

pi = c(.5, .5, .5, .5)

# impute missing outcomes
p1 = pi_11/(p1_11 + pi_10)
p2 = p1_01/(p1_01 + p1_00)
p3 = pi_11/(pi_01 + pi_11)
p4 = pi_10/(pi_10+pi_00)

dat$case = ifelse(dat$A ==1 & dat$Y1 ==1 , 1,
                  ifelse( dat$A == 1 & dat$Y1 == 0, 2,
                  ifelse( dat$A == 0 & dat$Y0 == 1, 3, 4) ) )





```

#### Up Next: Bayesian Network Framework



