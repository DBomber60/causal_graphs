---
title: "Ci"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
library(xtable)
library(igraph)
library(pander)
```

### Data

First let's revisit the example that we looked at previously from [this paper](https://tinyurl.com/y45qk8d4) where we have data from a treatment that occurs at two time points. The treatment at the first time point, $A0$, has a causal effect on the covariate $Z1$, which in turn has an effect on $A1$. All three have an effect on the outcome, $Y$.

```{r echo=FALSE}
# in order to use modelling, need to hypothesize a dose-response fn
A0 = c(0,0,0,0,1,1,1,1)
Z1 = c(0,0,1,1,0,0,1,1)
A1 = c(0,1,0,1,0,1,0,1)
Y = c(87.29, 112.11, 119.65, 144.84, 105.28, 130.18, 137.72, 162.83)
N = c(209271, 93779, 60654, 136293, 134781, 60789, 93903, 210527)
df = data.frame(A0=A0, Z1=Z1, A1=A1, Y=Y, N=N)
kable(df)
```

```{r}
# draw a graph of the effects using igraph
```

## Strategy 1: Marginalize out the effect of Z1

Assume a linear model for response $Y$ (correctly),

\begin{aligned}
E_{Y|A0,A1} &= E_{Z1|A0} \big(E(Y|A0,Z1,A1) \big) \\
&= E_{Z1|A0} \big( \beta_0 + \beta_1 A0 + \beta_2 Z1 + \beta_3 A1  \big)
\end{aligned}

Then, since Z1 is binary, we can model it using logistic regression where,


\begin{aligned}
E(Z1|A0) &= P(Z1=1 \vert A0 ) = \text{logit}^{-1} (\alpha_0 + \alpha_1 A0)\\
\end{aligned}

Since A0 is also binary, we can define the following two quantities,

\begin{aligned}
\xi_1 &= \text{logit}^{-1} (\alpha_0 + \alpha_1) \\
\xi_0 &= \text{logit}^{-1} (\alpha_0)
\end{aligned}

And then distribute the expectation to each term,

$$
\begin{aligned}
E_{Z1|A0} \big( \beta_0 + \beta_1 A0 + \beta_2 Z1 + \beta_3 A1  \big) &= \beta_0 + \beta_1 A0 + \beta_2 E(Z1|A0) + \beta_3 A1   \\
&= \beta_0 + \beta_1 A0 + \beta_2 \big( \xi_1 A0 + \xi_0(1-A0)) + \beta_3 A1 \\
&= (\beta_0+\beta_2 \xi_0) + (\beta_1 + \beta_2(\xi_1-\xi_0)) A0 + \beta_3 A1 \\
&= \phi_0 + \phi_1 A0 + \phi_2 A1
\end{aligned}
$$

This can all be done easily in R,

```{r}
# 1. get beta coefficients
m0 = lm(Y ~ A0 + Z1 + A1, weights = N) # E(Y| A0, Z1, A1)
m1 = glm(Z1 ~ A0, family = binomial, weights = N) # E(Z1 | A0)

xi_1 = plogis( sum(m1$coefficients) ) # xi_1
xi_0 = plogis( m1$coefficients[1] ) # xi_0

phi_0 = m0$coefficients[1] + m0$coefficients[3] * xi_0
phi_1 = m0$coefficients[2] + m0$coefficients[3] * (xi_1 - xi_0)
phi_2 = m0$coefficients[4]
```

As expected, $\phi_0 =$ `r round(phi_0,2)`; $\phi_1 =$ `r round(phi_1,2)`; $\phi_2 =$ `r round(phi_2,2)`.

However, this computation relied on:

- Binary treatment A0
- Linear model for outcome $Y$ (in order to bring the outer expectation inside).


## MSM via Inverse Probability Weighting 

In a 



$P(Y^a=y|A=a,L=l)=P(Y^a=y|A=a',L=l)$. That is, $Y^a \perp A | L$.  Or, within levels of patient risk, $L$, counterfactual outcomes are independent of the treatment received.
  \item \textbf{Consistency} - Values of treatment under comparison correspond to well-defined interventions that, in turn, correspond to the versions of treatment in the data. That is. $Y^a=Y$ when treatment $a$ has been observed, where $Y$ is the observed value of the outcome.
\end{enumerate}

Under these assumptions, we can think of our data as a conditionally random experiment where treatment is randomized within levels of $L$. That is, within the subset $L=l$, we have
\begin{equation*}
E(Y|A=a,L=l) = E(Y^a|L=l)



```{r echo=FALSE}

tm = glm(A1 ~ Z1, weights = N, family = binomial) # treatment model
tm_sw = glm(A1 ~ 1, weights = N, family = binomial) # treatment model - stabilize weights

sw_num = ifelse(df$A1==1, plogis(coefficients(tm_sw))*.5, (1-plogis(coefficients(tm_sw))) * .5 )

sw_den = ifelse(df$A1 == 1, fitted.values(tm), 1- fitted.values(tm)) * .5

pseudo_N = sw_num/sw_den * N

mod = lm(Y ~ A0 + A1, weights = pseudo_N)
pander(summary(mod))

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
